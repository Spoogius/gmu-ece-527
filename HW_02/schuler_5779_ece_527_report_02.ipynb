{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1392b34-c7f2-4bbd-a7bd-96e99bddf176",
   "metadata": {},
   "source": [
    "# GMU ECE 527 - Computer Exercise #2 - Report\n",
    "**Stewart Schuler - G01395779**\\\n",
    "**20240912**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6a0a6-198e-465c-a29b-f10ed9817792",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "#### Poacket Algorithm\n",
    "\n",
    "I modified the provided SDG alogrithm to implement the pocket alogirthm. *Figures 1* and *2* demonstrate the resulting decision boundary and accuracy scores. As expected the pocket alogirthm performed equal to, or better than SGD on both datasets.\\\n",
    "In order to make the algorithms work for the second dataset with minimal overlap, because that dataset has an offset from the origin the data sets needed to be modified to included an extra feature of all ones in order to learn the $\\omega_{0}$ bias.\n",
    "\n",
    "![Pocker Overlap](figures/SGD_v_Pocket_overlap.jpg)\\\n",
    "**Figure 1. SGD vs Pocket with Overlaping dataset**\n",
    "\n",
    "![Pocker Overlap](figures/SGD_v_Pocket_min_overlap.jpg)\\\n",
    "**Figure 2. SGD vs Pocket with Overlaping dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c21a6-8139-4459-92ee-3d9a2f4314ee",
   "metadata": {},
   "source": [
    "Q: Will PLA work with target values of y = 0 and 1, or is it necessary to convert them to y = Â±1? Does the Perceptron class care what the target values are?\n",
    "\n",
    "A: No PLA will not work with y values of 0/1. This is because when an updated to the $\\omega$ vector is triggered, it should update by $eta*X_{i}*y_{i}$. If $y_{i]$ is zero the the update step will also be zero in size, that is no change in $\\omega$.\\\n",
    "The *scikit-learn* implementation of Perception works equivilantly for *y* values of 0/1 or -1/1. *Figures 3-6* show the outputs of PLA and the Perceptron algorithm on the same dataset for both cases of *y* labels.\n",
    "\n",
    "![PLA -1/1](figures/PLA_-11.jpg)\\\n",
    "**Figure 3. PLA with y=-1/1**\n",
    "\n",
    "![PLA -1/1](figures/PLA_01.jpg)\\\n",
    "**Figure 4. PLA with y=-1/1**\n",
    "\n",
    "![PLA -1/1](figures/Perceptron_-11.jpg)\\\n",
    "**Figure 5. Perceptron with y=-1/1**\n",
    "\n",
    "![PLA -1/1](figures/Perceptron_-11.jpg)\\\n",
    "**Figure 6. Perceptron with y=-1/1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea8151-2689-4be0-8e3b-9d2866e14bba",
   "metadata": {},
   "source": [
    "## PLA Experiments\n",
    "\n",
    "### Dataset 1\n",
    "\n",
    "For dataset 1 because it is linearly separable PLA does converge. This can be seen by looking at the learning curve in *Figure 7*. When plotting the learning curve it was forced to run for 50 epochs, when letting the Perceptron class auto stop on convergence I found it to converge after 8 iterations. This was with $tol=5E-6$ but I found similar fast convergence for other tested values of tol. Since for the learner to auto stop the loss moss be stable for some number of epochs auto stopping at 8 means that the learner converges quickly and stays stable. Which the plotted learning curve agrees with.\n",
    "\n",
    "![Dataset 1 Learning Curve](figures/dataset_1_learning_curve.jpg)\\\n",
    "**Figure 7. Dataset 1 PLA learning curve**\n",
    "\n",
    "We see that the learned decision boundary performs quite well on the held out test set.\n",
    "![Dataset 1 Seed Variation](figures/dataset_1_pla.jpg)\\\n",
    "**Figure 8. Dataset 1 Test Set Result**\n",
    "\n",
    "*Figure 9* shows the affect of modifying the random seed for the class. It is demonstrated by the plot that the resulting converged upon decision boundary depends on the order of the data. \n",
    "\n",
    "![Dataset 1 Seed Variation](figures/dataset_1_seed_variation.jpg)\\\n",
    "**Figure 9. Dataset 1 Seed Variation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45fb3ef-3af3-4b7a-bbea-e43bfaf180be",
   "metadata": {},
   "source": [
    "### Dataset 2\n",
    "\n",
    "With again a tol value of $5E-6$ dataset 2 converged after 10 iteration. Like dataset 1 this happens because the loss is now longer being improved by subsequent epochs. However unlike dataset 1, because this dataset is **not** linearly separable it doesn't converge to a perfect accuracy. But rather some error. We can see that by chance when plotting the learning curve it happened to be stable from the first epoch onward. Testing with different random seeds produced other learning curved that were unstable, similar to the ones presented for datasets 3/4. \n",
    "\n",
    "![Dataset 2 Learning Curve](figures/dataset_2_learning_curve.jpg)\\\n",
    "**Figure 10. Dataset 2 PLA learning curve**\n",
    "\n",
    "The linear classifier performs okay on the held out test set. This is likely because the randomly chosen test set is not quite as non-linear as it could've potentially been randomly chosen to be.\\\n",
    "![Dataset 1 Seed Variation](figures/dataset_2_pla.jpg)\\\n",
    "**Figure 11. Dataset 1 Test Set Result**\n",
    "\n",
    "![Dataset 2 Seed Variation](figures/dataset_2_seed_variation.jpg)\\\n",
    "**Figure 12. Dataset 2 Seed Variation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1747d-09b4-41c7-9f65-8f71f58f218f",
   "metadata": {},
   "source": [
    "### Dataset 3\n",
    "\n",
    "With again a tol value of $5E-6$ dataset 2 auto stopped after 12 iteration. Because this dataset is highly non-linear it can be seen by the learning curve that it is never able to converge and fluctuates.\n",
    "\n",
    "![Dataset 3 Learning Curve](figures/dataset_3_learning_curve.jpg)\\\n",
    "**Figure 13. Dataset 3 PLA learning curve**\n",
    "\n",
    "\n",
    "Like with the non-linear dataset 2, the linear classifier does better than I would've expected. I believe this again happens because of the sparsity of the test dataset. If this testing set had a larger number of samples I would expect the accuracy to be lower and better represent the error of using the linear apporach on a non-linear dataset.\\\n",
    "![Dataset 3 Seed Variation](figures/dataset_3_pla.jpg)\\\n",
    "**Figure 14. Dataset 3 Test Set Result**\n",
    "\n",
    "![Dataset 3 Seed Variation](figures/dataset_3_seed_variation.jpg)\\\n",
    "**Figure 15. Dataset 3 Seed Variation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900f93e-870f-4a34-ba0e-9b6dabf068f3",
   "metadata": {},
   "source": [
    "### Dataset 4\n",
    "\n",
    "With again a tol value of $5E-6$ dataset 2 auto stopped after 6 iteration. Because this dataset is highly non-linear it can be seen by the learning curve that it is never able to converge and fluctuates.\n",
    "\n",
    "![Dataset 4 Learning Curve](figures/dataset_4_learning_curve.jpg)\\\n",
    "**Figure 16. Dataset 4 PLA learning curve**\n",
    "\n",
    "\n",
    "As expected the learned decision boundary doesn't represent the underlaying dataset at all. As such it has very poor accuracy.\n",
    "![Dataset 4 Seed Variation](figures/dataset_4_pla.jpg)\\\n",
    "**Figure 17. Dataset 4 Test Set Result**\n",
    "\n",
    "![Dataset 4 Seed Variation](figures/dataset_4_seed_variation.jpg)\\\n",
    "**Figure 18. Dataset 4 Seed Variation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e94243-5506-4cb5-81d8-c3a61b92d308",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "\n",
    "### Dataset 2\n",
    "For dataset 2, because it is know that the underlaying dataset is defined by a second order polynomial, the features were cast into this produces a dataset with 6 features. The resulting classifier scored a 0.9 accuracy. The learned decision boundary is shown in *Figure 19*. \n",
    "\n",
    "![Dataset 2 Non Linear Boundary](figures/dataset_2_poly.jpg)\\\n",
    "**Figure 19. Dataset 2 2nd order polynomial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fba65a-7c2e-49ab-8c86-9fa0db708dd8",
   "metadata": {},
   "source": [
    "### Dataset 3\n",
    "For dataset 3 I applied a 3rd order poly nomial feature space resulting in 10 features. Once trained it was found to have an accuracy of 0.99. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c226731-dd64-4143-9c2d-b9d85aeac5bc",
   "metadata": {},
   "source": [
    "### Dataset 4\n",
    "For dataset 4, because from prior knowledge we know the dataset is to circle with the same center point and different radii the ideal boundary would a circle centered at zero with a radius half way between the two class radii.\n",
    "\n",
    "For such a circular desired boundary the only three features needed would be\n",
    "$$\n",
    "z = [ x_{0}, x_{1}^{2}, x_{2}^{2} ]\n",
    "$$\n",
    "\n",
    "When applying these features the learned classifier has an accuracy of 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a6a4d-d6d6-47c6-a7ed-adf37c485827",
   "metadata": {},
   "source": [
    "### Non-linear decision to linear \n",
    "Applying non-linear features to the linear dataset I found that when using high order polynomials they would perform well, this is happening because the higher order polynomials can represent linear function by suppressing the high order portions. This is demonstrated by *Figure 20* which shows the learned decision boundary using second order polynomial features. It can be seen that the resulting boundary is very nearly linear. While unplotted the high accuracy for the 3rd order polynomial boundary sugguests the same the same thing is happening.\n",
    "\n",
    "![Dataset 1 Poly 2 Boundary](figures/dataset_1_poly_2.jpg)\\\n",
    "**Figure 20. Dataset 1 2nd order polynomail**\n",
    "\n",
    "Other tested non-linear feature spaces was the circle boundary used by Dataset 4 and a sin/cos feature space, which shouldn't be able to learn a linear classifier. The *z* for the sin/cos feature spaced was \\\n",
    "$$\n",
    "z = [ x_{0}, \\sin(x_{0}), \\cos(x_{1}) ]\n",
    "$$\n",
    "\n",
    "The result of the 4 classifiers on the linearly separable dataset 1 were as follows\n",
    "| Feature Space | Accuracy |\n",
    "| -------- | ------- |\n",
    "| 2nd Poly  | 0.98    |\n",
    "| 3rd Poly  | 1.00    |\n",
    "| Circle    | 0.98    |\n",
    "| Sin/Cos   | 0.49    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
