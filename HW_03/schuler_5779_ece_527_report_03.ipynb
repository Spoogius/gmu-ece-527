{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1392b34-c7f2-4bbd-a7bd-96e99bddf176",
   "metadata": {},
   "source": [
    "# GMU ECE 527 - Computer Exercise #3 - Report\n",
    "**Stewart Schuler - G01395779**\\\n",
    "**20240919**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef87be-158a-48bc-af58-e60b066d26e2",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "**Question:** How does the SVC class find the maximum-margin classifier?\n",
    "\n",
    "**Answer:** The SVC class applies the kernal via the kernel trick to the training data to create a higher dimentional dataset then uses the *dual problem* approach to solve the minimization probelm linearly in the higher dimentional space. When mapped back into the original feature space the decision boundary becomes non-linear (assuming a non-linear kernal was used).\n",
    "\n",
    "**Question:** There are two other classes in scikit-learn that may be used to find a maximum-margin\n",
    "classifier: LinearSVC and SGDClassifer. How are these different from SVC, how do they\n",
    "find the maximum-margin classifier, and when would you use these instead of the SVC\n",
    "class?\n",
    "\n",
    "**Answer:** *LinearSVC* solves the *primal problem* rather than the dual problem used by *SVC*. Solving this problem involves using matrix operation of size porportional to the number of features. When searching for a linear boundary it is assumed the number of features relativly low. Thus the *primal* approach will be more efficient than applying the kernel trick. The downside being for non-linear data the high dimention data transform must be applied directly which can exponentially increase the number of features making this approach slow.\\\n",
    "The *SGDClassifer* implments the same gradient decent algorithm as the *Perceptron* class with the notible change of using the different *hinge* loss function. In practice this function allows the algorithm to work on data that isn't strictly separable. The use case for this type of algorithm would be data that is near separable in a low dimentional feature space, and when there is a relativly small ammount of training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288fcfc-0c20-4b39-a2ab-3e7767a74cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee4d7f62-342a-485a-8bc3-409e3a876d3c",
   "metadata": {},
   "source": [
    "e) Repeat for a few other polynomial orders and discuss your findings\n",
    "\n",
    "2nd order poly is sufficient. Plots all make more or less the same curve. Data dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0d8c5-a942-4fc7-99e9-17f8a183cc41",
   "metadata": {},
   "source": [
    "(c) Experiment with some values of Î³ that you set manually.\n",
    "\n",
    "Note that 1/d and 1/(d*var) doesn't yeild best result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27def1a1-fac3-4fac-a40c-d313e248bf0a",
   "metadata": {},
   "source": [
    "Question\n",
    "Only one of these methods is the right one to use. Do some research to find out the right way\n",
    "to do the scaling and explain why this is the approach that should be used\n",
    "\n",
    "The second one. Scale based on training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
